{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Poetry Topic Modelling with Latent Dirichlet Allocation and Latent Semantic Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was utilised as part of my final year undergraduate project and dissertation, whose abstract is as follows:\n",
    "\n",
    "\n",
    "<b>Topic models, which detect latent themes in a corpus of documents to group co-occurring keywords together in thematically comprehensible ways, were generated using the Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) algorithms with three datasets of poetry from different time periods. A close reading of the results as well as a study to measure interpretability were used to measure which algorithm was the most successful at uncovering specific themes in each dataset established using relevant literary studies. Comparison between the two algorithms’ performances served to indicate which method was the most successful in modelling this highly figurative language. Our findings indicated that LDA generated the most thematically comprehensible topics, owing to improved performance in identifying context and polysemy in the vocabulary used throughout the corpora, as well as having more parameters available to tune and optimise performance. <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps taken to build our models in this notebook:\n",
    "1. Mine a large CSV file of poems from Kaggle to create smaller CSV files for each of the poetic movements we are exploring (Romantic, Metaphyiscal, Harlem Renaissance)\n",
    "\n",
    "2. Clean and preprocess the data, creating bigrams, dictionaries and document-term matrices ready to be passed into the Gensim model functions\n",
    "\n",
    "3. Evaluation and validation of topics generated. \n",
    "\n",
    "Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports for mining the Kaggle CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the original Kaggle dataset into a Pandas dataframe and making empty dataframes for each poetic movement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poetrydata = pd.read_csv('kaggle_poem_dataset.csv') #This is a csv containing many PoetryFoundation poems\n",
    "\n",
    "metaphysical = pd.DataFrame()\n",
    "romantic = pd.DataFrame()\n",
    "harlem = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function <b>addPoems</b> takes the main CSV and the name of a poet as input, and adds any poems from that poet into the dataframe we're building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPoems(poetrydata, poetname):\n",
    "    #Add every row (poem) whose author is the poet we specify into an object \n",
    "    newPoems = poetrydata[poetrydata['Author'].str.contains(poetname)]\n",
    "    return newPoems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lists for each poet whose works we want to explore are created for each movement. Identifying the poets whose work we would want to explore was done by searching for the poets in the Romantic and Harlem Renaissance movements as listed on poetryfoundation.org. Identifying the Metaphysical poets (a harder task) was done through researching several sources detailed further in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaphysicalPoets = ['John Donne', 'Abraham Cowley', 'Andrew Marvell', 'Richard Crashaw','George Herbert','John Cleveland', 'Henry Vaughan']\n",
    "romanticPoets = ['George Gordon','William Blake','Shelley','Felicia Dorothea Hemans','William Wordsworth','Coleridge','Keats','John Clare','Beddoes','William Lisle Bowles','Robert Burns','Barbauld','Heinrich Heine','Friedrich Hölderlin','Charles Lamb','Thomas Moore','Giacamo Leopardi','Christian Milne','Walter Scott','Robert Southey','Mary Lamb','Elizabeth Moody','Anna Seward','Elizabeth Bentley','Helen Leigh','George Crabbe','Joanna Baillie','Letitia Elizabeth Landon','Helen Maria Williams','Matilda Bethem','Mary Robinson','Walter Savage Landor','Leigh Hunt','Charlotte Smith','John Clare','Thomas Hood','Elizabeth Hands','Dorothy Wordsworth','Charlotte Richardson','Jane Taylor','Hartley Coleridge']\n",
    "harlemPoets = ['Langston Hughes','Paul Dunbar','Claude McKay','Melvin B. Tolson','James Weldon Johnson','Fenton Johnson','Countee Cullen','Anne Spencer','William Warning Cuney','Margaret Walker','Jean Toomer','Georgia Douglas Johnson','W. E. B. Du Bois','Arna Bontemps','Leslie Pickeny Hill','Sterling A. Brown','Alice Dunbar-Nelson','Jessie Redmon Fauset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using for loops and our <b>addPoems</b> function to build our DataFrames, then verifying their sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaphysical Poems: \n",
      " 126\n",
      "Harlem Renaissance Poems: \n",
      " 82\n",
      "Romantic Poems: \n",
      " 392\n"
     ]
    }
   ],
   "source": [
    "#Metaphysical\n",
    "for poet in metaphysicalPoets:\n",
    "    newPoems = addPoems(poetrydata, poet) \n",
    "    metaphysical = metaphysical.append(newPoems, ignore_index=True)\n",
    "\n",
    "#Romantic\n",
    "for poet in romanticPoets:\n",
    "    newPoems = addPoems(poetrydata, poet) \n",
    "    romantic = romantic.append(newPoems, ignore_index=True)\n",
    "\n",
    "#Harlem Renaissance\n",
    "for poet in harlemPoets:\n",
    "    newPoems = addPoems(poetrydata, poet) \n",
    "    harlem = harlem.append(newPoems, ignore_index=True)\n",
    "    \n",
    "    \n",
    "print('Metaphysical Poems: \\n', metaphysical.shape[0]) #shape[0] = row count = amount of poems\n",
    "print('Harlem Renaissance Poems: \\n', harlem.shape[0])\n",
    "print('Romantic Poems: \\n', romantic.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function createCSV takes the movement DataFrame and the name of the movement as parameters and is used to produce a CSV file for each movement's poems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCSV(movementDF, movementName):\n",
    "    content = pd.DataFrame(movementDF, columns=['Content'])#We only want the Content column (the poems)\n",
    "    content = content.replace('\\n',' ', regex=True) #Remove line breaks for formatting\n",
    "    export_csv = content.to_csv(r\"\"+movementName+\".csv\", index = None, header=True) \n",
    "\n",
    "createCSV(romantic, 'romantic')\n",
    "createCSV(metaphysical, 'metaphysical')\n",
    "createCSV(harlem,'harlem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our datasets prepared, we will create the LDA and LSA models.\n",
    "\n",
    "Step 2:\n",
    "First we'll import the necessary libraries, making it clear which modules we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora, models, utils\n",
    "from gensim import similarities\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "import string\n",
    "import re #regex\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LsiModel\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll specify the movement we wish to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the poetry movement (romantic, metaphysical, harlem)\n",
      "romantic\n"
     ]
    }
   ],
   "source": [
    "poemSet = input('Please specify the poetry movement (romantic, metaphysical, harlem)\\n')\n",
    "poems = pd.read_csv(poemSet+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will preprocess the data ready for bigram models and a document-term matrix to be built from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\w\n",
      "<ipython-input-176-e9b05989c589>:7: DeprecationWarning: invalid escape sequence \\w\n",
      "  poems[\"poems_processed\"] = poems['Content'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(poems):\n",
    "    for poem in poems:\n",
    "        yield(gensim.utils.simple_preprocess(str(poem))) #For formatting \n",
    "\n",
    "\n",
    "#Remove punctuation\n",
    "poems[\"poems_processed\"] = poems['Content'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#Make all lowercase\n",
    "poems[\"poems_processed\"] = poems['poems_processed'].str.lower()\n",
    "\n",
    "#Remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "#Many of our poems contain some antiquated language not accounted for in NLTK's stopwords collection, so we need \n",
    "#to add them. A few other words have been added which consistently made their way into almost every topic and needed\n",
    "#to be processed out (these words are not very substantive anyway)\n",
    "stop.extend(['from', 'like', 'thou', 'may', 'much','let','ye','said','tis','thy','whose','thee','yet','shall','one', 'see','every','amp','even','juan','yarrow','upon','though','oh'])\n",
    "poems['poems_processed'] = poems['poems_processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#Put column of poems into new variable\n",
    "poemtemp = poems['poems_processed']\n",
    "\n",
    "#Conversion of the processed column into its own dataframe then a list to keep formatting\n",
    "datadf = poemtemp.to_frame() \n",
    "data = datadf['poems_processed'].values.tolist()\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions we'll use to format and create bigrams from our corpus respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams, a dictionary, preparing the corpus and finally a document-term matrix to pass into the model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the LDA model using MALLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.073*\"love\" + 0.016*\"tears\" + 0.014*\"heart\" + 0.014*\"art\" + 0.013*\"smile\" '\n",
      "  '+ 0.013*\"voice\" + 0.010*\"weep\" + 0.009*\"fair\" + 0.009*\"divine\" + '\n",
      "  '0.009*\"soft\" + 0.008*\"sleep\" + 0.008*\"silent\" + 0.008*\"grace\" + '\n",
      "  '0.008*\"dost\" + 0.008*\"live\" + 0.008*\"head\" + 0.008*\"flower\" + '\n",
      "  '0.007*\"gentle\" + 0.007*\"peace\" + 0.007*\"face\"'),\n",
      " (1,\n",
      "  '0.032*\"sweet\" + 0.017*\"hear\" + 0.017*\"green\" + 0.015*\"spring\" + '\n",
      "  '0.012*\"sing\" + 0.010*\"grass\" + 0.010*\"happy\" + 0.009*\"flowers\" + '\n",
      "  '0.009*\"die\" + 0.009*\"fancy\" + 0.009*\"birds\" + 0.009*\"sun\" + 0.008*\"warm\" + '\n",
      "  '0.008*\"bring\" + 0.008*\"lie\" + 0.007*\"song\" + 0.007*\"nest\" + 0.006*\"hath\" + '\n",
      "  '0.006*\"bird\" + 0.006*\"golden\"'),\n",
      " (2,\n",
      "  '0.022*\"eyes\" + 0.015*\"sweet\" + 0.012*\"lady\" + 0.010*\"face\" + 0.010*\"heart\" '\n",
      "  '+ 0.009*\"rose\" + 0.009*\"fair\" + 0.008*\"side\" + 0.008*\"eye\" + 0.008*\"hath\" + '\n",
      "  '0.008*\"long\" + 0.008*\"bright\" + 0.008*\"white\" + 0.007*\"arms\" + '\n",
      "  '0.007*\"night\" + 0.006*\"child\" + 0.006*\"gentle\" + 0.006*\"maid\" + '\n",
      "  '0.006*\"full\" + 0.006*\"lips\"'),\n",
      " (3,\n",
      "  '0.022*\"light\" + 0.013*\"death\" + 0.013*\"night\" + 0.011*\"eyes\" + '\n",
      "  '0.010*\"heaven\" + 0.010*\"life\" + 0.009*\"wings\" + 0.009*\"air\" + 0.008*\"feet\" '\n",
      "  '+ 0.008*\"dead\" + 0.008*\"day\" + 0.008*\"earth\" + 0.008*\"men\" + 0.007*\"pale\" + '\n",
      "  '0.007*\"lost\" + 0.007*\"thought\" + 0.007*\"shape\" + 0.006*\"dream\" + '\n",
      "  '0.006*\"brow\" + 0.006*\"soft\"'),\n",
      " (4,\n",
      "  '0.012*\"beneath\" + 0.011*\"stream\" + 0.010*\"dark\" + 0.010*\"deep\" + '\n",
      "  '0.009*\"clouds\" + 0.009*\"calm\" + 0.009*\"mighty\" + 0.008*\"sound\" + '\n",
      "  '0.008*\"hills\" + 0.007*\"woods\" + 0.007*\"world\" + 0.007*\"moon\" + '\n",
      "  '0.006*\"human\" + 0.006*\"spread\" + 0.006*\"evening\" + 0.006*\"voice\" + '\n",
      "  '0.006*\"oer\" + 0.006*\"hours\" + 0.006*\"solemn\" + 0.006*\"shade\"'),\n",
      " (5,\n",
      "  '0.018*\"oer\" + 0.013*\"long\" + 0.011*\"pain\" + 0.010*\"vain\" + 0.009*\"free\" + '\n",
      "  '0.008*\"pride\" + 0.008*\"hope\" + 0.007*\"low\" + 0.007*\"grave\" + 0.007*\"slow\" + '\n",
      "  '0.007*\"blind\" + 0.007*\"age\" + 0.007*\"care\" + 0.006*\"hour\" + 0.006*\"proud\" + '\n",
      "  '0.006*\"bear\" + 0.006*\"scorn\" + 0.005*\"woe\" + 0.005*\"amid\" + 0.005*\"fate\"'),\n",
      " (6,\n",
      "  '0.018*\"life\" + 0.018*\"mind\" + 0.013*\"nature\" + 0.013*\"joy\" + '\n",
      "  '0.013*\"thoughts\" + 0.012*\"power\" + 0.011*\"days\" + 0.010*\"heart\" + '\n",
      "  '0.010*\"spirit\" + 0.009*\"things\" + 0.009*\"soul\" + 0.009*\"thought\" + '\n",
      "  '0.009*\"time\" + 0.009*\"dear\" + 0.008*\"human\" + 0.008*\"hath\" + 0.008*\"friend\" '\n",
      "  '+ 0.008*\"day\" + 0.008*\"earth\" + 0.007*\"happy\"'),\n",
      " (7,\n",
      "  '0.018*\"meet\" + 0.018*\"wi\" + 0.014*\"ill\" + 0.010*\"dear\" + 0.010*\"thro\" + '\n",
      "  '0.008*\"till\" + 0.008*\"poor\" + 0.008*\"night\" + 0.007*\"sae\" + 0.007*\"bonie\" + '\n",
      "  '0.006*\"tam\" + 0.006*\"body\" + 0.006*\"im\" + 0.006*\"tho\" + 0.005*\"frae\" + '\n",
      "  '0.005*\"auld\" + 0.005*\"ha_ha\" + 0.005*\"wooin_ot\" + 0.005*\"honest\" + '\n",
      "  '0.004*\"fare\"'),\n",
      " (8,\n",
      "  '0.024*\"man\" + 0.017*\"hand\" + 0.013*\"men\" + 0.011*\"good\" + 0.011*\"head\" + '\n",
      "  '0.009*\"poor\" + 0.009*\"god\" + 0.009*\"ere\" + 0.009*\"door\" + 0.008*\"found\" + '\n",
      "  '0.008*\"left\" + 0.008*\"wide\" + 0.007*\"young\" + 0.007*\"land\" + 0.007*\"youth\" '\n",
      "  '+ 0.006*\"rich\" + 0.006*\"water\" + 0.006*\"made\" + 0.006*\"heard\" + '\n",
      "  '0.006*\"fly\"'),\n",
      " (9,\n",
      "  '0.010*\"joy\" + 0.010*\"oer\" + 0.009*\"joys\" + 0.008*\"eye\" + 0.008*\"early\" + '\n",
      "  '0.007*\"song\" + 0.007*\"rude\" + 0.007*\"gay\" + 0.007*\"wild\" + 0.007*\"round\" + '\n",
      "  '0.006*\"pleasure\" + 0.006*\"morn\" + 0.006*\"sound\" + 0.006*\"vain\" + '\n",
      "  '0.006*\"natures\" + 0.006*\"heart\" + 0.005*\"muse\" + 0.005*\"sweet\" + '\n",
      "  '0.005*\"makes\" + 0.005*\"trace\"'),\n",
      " (10,\n",
      "  '0.020*\"day\" + 0.013*\"round\" + 0.011*\"time\" + 0.011*\"breast\" + 0.010*\"child\" '\n",
      "  '+ 0.010*\"years\" + 0.009*\"fair\" + 0.008*\"ground\" + 0.008*\"eye\" + '\n",
      "  '0.008*\"born\" + 0.008*\"left\" + 0.007*\"misery\" + 0.007*\"beneath\" + '\n",
      "  '0.007*\"heard\" + 0.007*\"mothers\" + 0.007*\"hill\" + 0.007*\"father\" + '\n",
      "  '0.007*\"laid\" + 0.006*\"find\" + 0.006*\"twas\"'),\n",
      " (11,\n",
      "  '0.018*\"sea\" + 0.015*\"sky\" + 0.013*\"sun\" + 0.013*\"till\" + 0.013*\"light\" + '\n",
      "  '0.013*\"blue\" + 0.012*\"high\" + 0.012*\"sleep\" + 0.012*\"air\" + 0.011*\"earth\" + '\n",
      "  '0.010*\"night\" + 0.010*\"heaven\" + 0.009*\"wind\" + 0.009*\"made\" + '\n",
      "  '0.009*\"mountains\" + 0.008*\"wild\" + 0.008*\"living\" + 0.008*\"ocean\" + '\n",
      "  '0.008*\"cold\" + 0.008*\"green\"'),\n",
      " (12,\n",
      "  '0.010*\"time\" + 0.008*\"late\" + 0.007*\"find\" + 0.007*\"ill\" + 0.006*\"great\" + '\n",
      "  '0.006*\"read\" + 0.006*\"die\" + 0.006*\"turn\" + 0.005*\"people\" + 0.005*\"ive\" + '\n",
      "  '0.005*\"set\" + 0.005*\"true\" + 0.005*\"truth\" + 0.004*\"praise\" + 0.004*\"world\" '\n",
      "  '+ 0.004*\"young\" + 0.004*\"good\" + 0.004*\"lie\" + 0.004*\"sir\" + '\n",
      "  '0.004*\"native\"')]\n"
     ]
    }
   ],
   "source": [
    "mallet_path = '/Users/admin/mallet-2.0.8/bin/mallet' #path to mallet\n",
    "\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=13, id2word=id2word,\n",
    "                                                random_seed=100, iterations=500) #Increase iterations for improvement\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(num_topics=-1, num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll compute the Coherence score as a quantitative performance metric (as detailed in the report, this does not really tell us much in this project and seems essentially arbitrary, but will be included for demonstration purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3641519126631765\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_words_bigrams, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll perform LSA, by first taking as input the name of the poetic movement to be explored (we will assume for the purposes of this notebook's usability that it will be the same as the one specified previously for LDA) and setting up some other variables. LSA won't work as intuitively using DataFrames so we'll prepare the poems in a list instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \n",
    "    documents_list = []\n",
    "    titles=[]\n",
    "    poems = pd.read_csv(file_name+'.csv')\n",
    "    poemnum = poems.shape[0]\n",
    "    print(\"Poemnum = \",poemnum)\n",
    "\n",
    "    \n",
    "    for i in range(poemnum-1):\n",
    "        forname = str(i)\n",
    "        file = open(poemSet+\"Txt\"+ \"/Poem\" +forname+\".txt\", \"r\") \n",
    "        #print(\"Currently exploring poem \" , i )\n",
    "        for line in file.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "\n",
    "        file.close() \n",
    "        print('Run once')\n",
    "\n",
    "    print(\"Total Number of Documents:\",len(documents_list))\n",
    "    #titles.append( text[0:min(len(text),100)] )\n",
    "    return documents_list\n",
    "\n",
    "# LSA Model\n",
    "number_of_topics=13\n",
    "words=20\n",
    "#poemSetLSA = poemSet as defined earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll preprocess this new list in the same way, with the same stopwords in order to convert into a new document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poemnum =  392\n",
      "Currently exploring poem  0\n",
      "Run once\n",
      "Currently exploring poem  1\n",
      "Run once\n",
      "Currently exploring poem  2\n",
      "Run once\n",
      "Currently exploring poem  3\n",
      "Run once\n",
      "Currently exploring poem  4\n",
      "Run once\n",
      "Currently exploring poem  5\n",
      "Run once\n",
      "Currently exploring poem  6\n",
      "Run once\n",
      "Currently exploring poem  7\n",
      "Run once\n",
      "Currently exploring poem  8\n",
      "Run once\n",
      "Currently exploring poem  9\n",
      "Run once\n",
      "Currently exploring poem  10\n",
      "Run once\n",
      "Currently exploring poem  11\n",
      "Run once\n",
      "Currently exploring poem  12\n",
      "Run once\n",
      "Currently exploring poem  13\n",
      "Run once\n",
      "Currently exploring poem  14\n",
      "Run once\n",
      "Currently exploring poem  15\n",
      "Run once\n",
      "Currently exploring poem  16\n",
      "Run once\n",
      "Currently exploring poem  17\n",
      "Run once\n",
      "Currently exploring poem  18\n",
      "Run once\n",
      "Currently exploring poem  19\n",
      "Run once\n",
      "Currently exploring poem  20\n",
      "Run once\n",
      "Currently exploring poem  21\n",
      "Run once\n",
      "Currently exploring poem  22\n",
      "Run once\n",
      "Currently exploring poem  23\n",
      "Run once\n",
      "Currently exploring poem  24\n",
      "Run once\n",
      "Currently exploring poem  25\n",
      "Run once\n",
      "Currently exploring poem  26\n",
      "Run once\n",
      "Currently exploring poem  27\n",
      "Run once\n",
      "Currently exploring poem  28\n",
      "Run once\n",
      "Currently exploring poem  29\n",
      "Run once\n",
      "Currently exploring poem  30\n",
      "Run once\n",
      "Currently exploring poem  31\n",
      "Run once\n",
      "Currently exploring poem  32\n",
      "Run once\n",
      "Currently exploring poem  33\n",
      "Run once\n",
      "Currently exploring poem  34\n",
      "Run once\n",
      "Currently exploring poem  35\n",
      "Run once\n",
      "Currently exploring poem  36\n",
      "Run once\n",
      "Currently exploring poem  37\n",
      "Run once\n",
      "Currently exploring poem  38\n",
      "Run once\n",
      "Currently exploring poem  39\n",
      "Run once\n",
      "Currently exploring poem  40\n",
      "Run once\n",
      "Currently exploring poem  41\n",
      "Run once\n",
      "Currently exploring poem  42\n",
      "Run once\n",
      "Currently exploring poem  43\n",
      "Run once\n",
      "Currently exploring poem  44\n",
      "Run once\n",
      "Currently exploring poem  45\n",
      "Run once\n",
      "Currently exploring poem  46\n",
      "Run once\n",
      "Currently exploring poem  47\n",
      "Run once\n",
      "Currently exploring poem  48\n",
      "Run once\n",
      "Currently exploring poem  49\n",
      "Run once\n",
      "Currently exploring poem  50\n",
      "Run once\n",
      "Currently exploring poem  51\n",
      "Run once\n",
      "Currently exploring poem  52\n",
      "Run once\n",
      "Currently exploring poem  53\n",
      "Run once\n",
      "Currently exploring poem  54\n",
      "Run once\n",
      "Currently exploring poem  55\n",
      "Run once\n",
      "Currently exploring poem  56\n",
      "Run once\n",
      "Currently exploring poem  57\n",
      "Run once\n",
      "Currently exploring poem  58\n",
      "Run once\n",
      "Currently exploring poem  59\n",
      "Run once\n",
      "Currently exploring poem  60\n",
      "Run once\n",
      "Currently exploring poem  61\n",
      "Run once\n",
      "Currently exploring poem  62\n",
      "Run once\n",
      "Currently exploring poem  63\n",
      "Run once\n",
      "Currently exploring poem  64\n",
      "Run once\n",
      "Currently exploring poem  65\n",
      "Run once\n",
      "Currently exploring poem  66\n",
      "Run once\n",
      "Currently exploring poem  67\n",
      "Run once\n",
      "Currently exploring poem  68\n",
      "Run once\n",
      "Currently exploring poem  69\n",
      "Run once\n",
      "Currently exploring poem  70\n",
      "Run once\n",
      "Currently exploring poem  71\n",
      "Run once\n",
      "Currently exploring poem  72\n",
      "Run once\n",
      "Currently exploring poem  73\n",
      "Run once\n",
      "Currently exploring poem  74\n",
      "Run once\n",
      "Currently exploring poem  75\n",
      "Run once\n",
      "Currently exploring poem  76\n",
      "Run once\n",
      "Currently exploring poem  77\n",
      "Run once\n",
      "Currently exploring poem  78\n",
      "Run once\n",
      "Currently exploring poem  79\n",
      "Run once\n",
      "Currently exploring poem  80\n",
      "Run once\n",
      "Currently exploring poem  81\n",
      "Run once\n",
      "Currently exploring poem  82\n",
      "Run once\n",
      "Currently exploring poem  83\n",
      "Run once\n",
      "Currently exploring poem  84\n",
      "Run once\n",
      "Currently exploring poem  85\n",
      "Run once\n",
      "Currently exploring poem  86\n",
      "Run once\n",
      "Currently exploring poem  87\n",
      "Run once\n",
      "Currently exploring poem  88\n",
      "Run once\n",
      "Currently exploring poem  89\n",
      "Run once\n",
      "Currently exploring poem  90\n",
      "Run once\n",
      "Currently exploring poem  91\n",
      "Run once\n",
      "Currently exploring poem  92\n",
      "Run once\n",
      "Currently exploring poem  93\n",
      "Run once\n",
      "Currently exploring poem  94\n",
      "Run once\n",
      "Currently exploring poem  95\n",
      "Run once\n",
      "Currently exploring poem  96\n",
      "Run once\n",
      "Currently exploring poem  97\n",
      "Run once\n",
      "Currently exploring poem  98\n",
      "Run once\n",
      "Currently exploring poem  99\n",
      "Run once\n",
      "Currently exploring poem  100\n",
      "Run once\n",
      "Currently exploring poem  101\n",
      "Run once\n",
      "Currently exploring poem  102\n",
      "Run once\n",
      "Currently exploring poem  103\n",
      "Run once\n",
      "Currently exploring poem  104\n",
      "Run once\n",
      "Currently exploring poem  105\n",
      "Run once\n",
      "Currently exploring poem  106\n",
      "Run once\n",
      "Currently exploring poem  107\n",
      "Run once\n",
      "Currently exploring poem  108\n",
      "Run once\n",
      "Currently exploring poem  109\n",
      "Run once\n",
      "Currently exploring poem  110\n",
      "Run once\n",
      "Currently exploring poem  111\n",
      "Run once\n",
      "Currently exploring poem  112\n",
      "Run once\n",
      "Currently exploring poem  113\n",
      "Run once\n",
      "Currently exploring poem  114\n",
      "Run once\n",
      "Currently exploring poem  115\n",
      "Run once\n",
      "Currently exploring poem  116\n",
      "Run once\n",
      "Currently exploring poem  117\n",
      "Run once\n",
      "Currently exploring poem  118\n",
      "Run once\n",
      "Currently exploring poem  119\n",
      "Run once\n",
      "Currently exploring poem  120\n",
      "Run once\n",
      "Currently exploring poem  121\n",
      "Run once\n",
      "Currently exploring poem  122\n",
      "Run once\n",
      "Currently exploring poem  123\n",
      "Run once\n",
      "Currently exploring poem  124\n",
      "Run once\n",
      "Currently exploring poem  125\n",
      "Run once\n",
      "Currently exploring poem  126\n",
      "Run once\n",
      "Currently exploring poem  127\n",
      "Run once\n",
      "Currently exploring poem  128\n",
      "Run once\n",
      "Currently exploring poem  129\n",
      "Run once\n",
      "Currently exploring poem  130\n",
      "Run once\n",
      "Currently exploring poem  131\n",
      "Run once\n",
      "Currently exploring poem  132\n",
      "Run once\n",
      "Currently exploring poem  133\n",
      "Run once\n",
      "Currently exploring poem  134\n",
      "Run once\n",
      "Currently exploring poem  135\n",
      "Run once\n",
      "Currently exploring poem  136\n",
      "Run once\n",
      "Currently exploring poem  137\n",
      "Run once\n",
      "Currently exploring poem  138\n",
      "Run once\n",
      "Currently exploring poem  139\n",
      "Run once\n",
      "Currently exploring poem  140\n",
      "Run once\n",
      "Currently exploring poem  141\n",
      "Run once\n",
      "Currently exploring poem  142\n",
      "Run once\n",
      "Currently exploring poem  143\n",
      "Run once\n",
      "Currently exploring poem  144\n",
      "Run once\n",
      "Currently exploring poem  145\n",
      "Run once\n",
      "Currently exploring poem  146\n",
      "Run once\n",
      "Currently exploring poem  147\n",
      "Run once\n",
      "Currently exploring poem  148\n",
      "Run once\n",
      "Currently exploring poem  149\n",
      "Run once\n",
      "Currently exploring poem  150\n",
      "Run once\n",
      "Currently exploring poem  151\n",
      "Run once\n",
      "Currently exploring poem  152\n",
      "Run once\n",
      "Currently exploring poem  153\n",
      "Run once\n",
      "Currently exploring poem  154\n",
      "Run once\n",
      "Currently exploring poem  155\n",
      "Run once\n",
      "Currently exploring poem  156\n",
      "Run once\n",
      "Currently exploring poem  157\n",
      "Run once\n",
      "Currently exploring poem  158\n",
      "Run once\n",
      "Currently exploring poem  159\n",
      "Run once\n",
      "Currently exploring poem  160\n",
      "Run once\n",
      "Currently exploring poem  161\n",
      "Run once\n",
      "Currently exploring poem  162\n",
      "Run once\n",
      "Currently exploring poem  163\n",
      "Run once\n",
      "Currently exploring poem  164\n",
      "Run once\n",
      "Currently exploring poem  165\n",
      "Run once\n",
      "Currently exploring poem  166\n",
      "Run once\n",
      "Currently exploring poem  167\n",
      "Run once\n",
      "Currently exploring poem  168\n",
      "Run once\n",
      "Currently exploring poem  169\n",
      "Run once\n",
      "Currently exploring poem  170\n",
      "Run once\n",
      "Currently exploring poem  171\n",
      "Run once\n",
      "Currently exploring poem  172\n",
      "Run once\n",
      "Currently exploring poem  173\n",
      "Run once\n",
      "Currently exploring poem  174\n",
      "Run once\n",
      "Currently exploring poem  175\n",
      "Run once\n",
      "Currently exploring poem  176\n",
      "Run once\n",
      "Currently exploring poem  177\n",
      "Run once\n",
      "Currently exploring poem  178\n",
      "Run once\n",
      "Currently exploring poem  179\n",
      "Run once\n",
      "Currently exploring poem  180\n",
      "Run once\n",
      "Currently exploring poem  181\n",
      "Run once\n",
      "Currently exploring poem  182\n",
      "Run once\n",
      "Currently exploring poem  183\n",
      "Run once\n",
      "Currently exploring poem  184\n",
      "Run once\n",
      "Currently exploring poem  185\n",
      "Run once\n",
      "Currently exploring poem  186\n",
      "Run once\n",
      "Currently exploring poem  187\n",
      "Run once\n",
      "Currently exploring poem  188\n",
      "Run once\n",
      "Currently exploring poem  189\n",
      "Run once\n",
      "Currently exploring poem  190\n",
      "Run once\n",
      "Currently exploring poem  191\n",
      "Run once\n",
      "Currently exploring poem  192\n",
      "Run once\n",
      "Currently exploring poem  193\n",
      "Run once\n",
      "Currently exploring poem  194\n",
      "Run once\n",
      "Currently exploring poem  195\n",
      "Run once\n",
      "Currently exploring poem  196\n",
      "Run once\n",
      "Currently exploring poem  197\n",
      "Run once\n",
      "Currently exploring poem  198\n",
      "Run once\n",
      "Currently exploring poem  199\n",
      "Run once\n",
      "Currently exploring poem  200\n",
      "Run once\n",
      "Currently exploring poem  201\n",
      "Run once\n",
      "Currently exploring poem  202\n",
      "Run once\n",
      "Currently exploring poem  203\n",
      "Run once\n",
      "Currently exploring poem  204\n",
      "Run once\n",
      "Currently exploring poem  205\n",
      "Run once\n",
      "Currently exploring poem  206\n",
      "Run once\n",
      "Currently exploring poem  207\n",
      "Run once\n",
      "Currently exploring poem  208\n",
      "Run once\n",
      "Currently exploring poem  209\n",
      "Run once\n",
      "Currently exploring poem  210\n",
      "Run once\n",
      "Currently exploring poem  211\n",
      "Run once\n",
      "Currently exploring poem  212\n",
      "Run once\n",
      "Currently exploring poem  213\n",
      "Run once\n",
      "Currently exploring poem  214\n",
      "Run once\n",
      "Currently exploring poem  215\n",
      "Run once\n",
      "Currently exploring poem  216\n",
      "Run once\n",
      "Currently exploring poem  217\n",
      "Run once\n",
      "Currently exploring poem  218\n",
      "Run once\n",
      "Currently exploring poem  219\n",
      "Run once\n",
      "Currently exploring poem  220\n",
      "Run once\n",
      "Currently exploring poem  221\n",
      "Run once\n",
      "Currently exploring poem  222\n",
      "Run once\n",
      "Currently exploring poem  223\n",
      "Run once\n",
      "Currently exploring poem  224\n",
      "Run once\n",
      "Currently exploring poem  225\n",
      "Run once\n",
      "Currently exploring poem  226\n",
      "Run once\n",
      "Currently exploring poem  227\n",
      "Run once\n",
      "Currently exploring poem  228\n",
      "Run once\n",
      "Currently exploring poem  229\n",
      "Run once\n",
      "Currently exploring poem  230\n",
      "Run once\n",
      "Currently exploring poem  231\n",
      "Run once\n",
      "Currently exploring poem  232\n",
      "Run once\n",
      "Currently exploring poem  233\n",
      "Run once\n",
      "Currently exploring poem  234\n",
      "Run once\n",
      "Currently exploring poem  235\n",
      "Run once\n",
      "Currently exploring poem  236\n",
      "Run once\n",
      "Currently exploring poem  237\n",
      "Run once\n",
      "Currently exploring poem  238\n",
      "Run once\n",
      "Currently exploring poem  239\n",
      "Run once\n",
      "Currently exploring poem  240\n",
      "Run once\n",
      "Currently exploring poem  241\n",
      "Run once\n",
      "Currently exploring poem  242\n",
      "Run once\n",
      "Currently exploring poem  243\n",
      "Run once\n",
      "Currently exploring poem  244\n",
      "Run once\n",
      "Currently exploring poem  245\n",
      "Run once\n",
      "Currently exploring poem  246\n",
      "Run once\n",
      "Currently exploring poem  247\n",
      "Run once\n",
      "Currently exploring poem  248\n",
      "Run once\n",
      "Currently exploring poem  249\n",
      "Run once\n",
      "Currently exploring poem  250\n",
      "Run once\n",
      "Currently exploring poem  251\n",
      "Run once\n",
      "Currently exploring poem  252\n",
      "Run once\n",
      "Currently exploring poem  253\n",
      "Run once\n",
      "Currently exploring poem  254\n",
      "Run once\n",
      "Currently exploring poem  255\n",
      "Run once\n",
      "Currently exploring poem  256\n",
      "Run once\n",
      "Currently exploring poem  257\n",
      "Run once\n",
      "Currently exploring poem  258\n",
      "Run once\n",
      "Currently exploring poem  259\n",
      "Run once\n",
      "Currently exploring poem  260\n",
      "Run once\n",
      "Currently exploring poem  261\n",
      "Run once\n",
      "Currently exploring poem  262\n",
      "Run once\n",
      "Currently exploring poem  263\n",
      "Run once\n",
      "Currently exploring poem  264\n",
      "Run once\n",
      "Currently exploring poem  265\n",
      "Run once\n",
      "Currently exploring poem  266\n",
      "Run once\n",
      "Currently exploring poem  267\n",
      "Run once\n",
      "Currently exploring poem  268\n",
      "Run once\n",
      "Currently exploring poem  269\n",
      "Run once\n",
      "Currently exploring poem  270\n",
      "Run once\n",
      "Currently exploring poem  271\n",
      "Run once\n",
      "Currently exploring poem  272\n",
      "Run once\n",
      "Currently exploring poem  273\n",
      "Run once\n",
      "Currently exploring poem  274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run once\n",
      "Currently exploring poem  275\n",
      "Run once\n",
      "Currently exploring poem  276\n",
      "Run once\n",
      "Currently exploring poem  277\n",
      "Run once\n",
      "Currently exploring poem  278\n",
      "Run once\n",
      "Currently exploring poem  279\n",
      "Run once\n",
      "Currently exploring poem  280\n",
      "Run once\n",
      "Currently exploring poem  281\n",
      "Run once\n",
      "Currently exploring poem  282\n",
      "Run once\n",
      "Currently exploring poem  283\n",
      "Run once\n",
      "Currently exploring poem  284\n",
      "Run once\n",
      "Currently exploring poem  285\n",
      "Run once\n",
      "Currently exploring poem  286\n",
      "Run once\n",
      "Currently exploring poem  287\n",
      "Run once\n",
      "Currently exploring poem  288\n",
      "Run once\n",
      "Currently exploring poem  289\n",
      "Run once\n",
      "Currently exploring poem  290\n",
      "Run once\n",
      "Currently exploring poem  291\n",
      "Run once\n",
      "Currently exploring poem  292\n",
      "Run once\n",
      "Currently exploring poem  293\n",
      "Run once\n",
      "Currently exploring poem  294\n",
      "Run once\n",
      "Currently exploring poem  295\n",
      "Run once\n",
      "Currently exploring poem  296\n",
      "Run once\n",
      "Currently exploring poem  297\n",
      "Run once\n",
      "Currently exploring poem  298\n",
      "Run once\n",
      "Currently exploring poem  299\n",
      "Run once\n",
      "Currently exploring poem  300\n",
      "Run once\n",
      "Currently exploring poem  301\n",
      "Run once\n",
      "Currently exploring poem  302\n",
      "Run once\n",
      "Currently exploring poem  303\n",
      "Run once\n",
      "Currently exploring poem  304\n",
      "Run once\n",
      "Currently exploring poem  305\n",
      "Run once\n",
      "Currently exploring poem  306\n",
      "Run once\n",
      "Currently exploring poem  307\n",
      "Run once\n",
      "Currently exploring poem  308\n",
      "Run once\n",
      "Currently exploring poem  309\n",
      "Run once\n",
      "Currently exploring poem  310\n",
      "Run once\n",
      "Currently exploring poem  311\n",
      "Run once\n",
      "Currently exploring poem  312\n",
      "Run once\n",
      "Currently exploring poem  313\n",
      "Run once\n",
      "Currently exploring poem  314\n",
      "Run once\n",
      "Currently exploring poem  315\n",
      "Run once\n",
      "Currently exploring poem  316\n",
      "Run once\n",
      "Currently exploring poem  317\n",
      "Run once\n",
      "Currently exploring poem  318\n",
      "Run once\n",
      "Currently exploring poem  319\n",
      "Run once\n",
      "Currently exploring poem  320\n",
      "Run once\n",
      "Currently exploring poem  321\n",
      "Run once\n",
      "Currently exploring poem  322\n",
      "Run once\n",
      "Currently exploring poem  323\n",
      "Run once\n",
      "Currently exploring poem  324\n",
      "Run once\n",
      "Currently exploring poem  325\n",
      "Run once\n",
      "Currently exploring poem  326\n",
      "Run once\n",
      "Currently exploring poem  327\n",
      "Run once\n",
      "Currently exploring poem  328\n",
      "Run once\n",
      "Currently exploring poem  329\n",
      "Run once\n",
      "Currently exploring poem  330\n",
      "Run once\n",
      "Currently exploring poem  331\n",
      "Run once\n",
      "Currently exploring poem  332\n",
      "Run once\n",
      "Currently exploring poem  333\n",
      "Run once\n",
      "Currently exploring poem  334\n",
      "Run once\n",
      "Currently exploring poem  335\n",
      "Run once\n",
      "Currently exploring poem  336\n",
      "Run once\n",
      "Currently exploring poem  337\n",
      "Run once\n",
      "Currently exploring poem  338\n",
      "Run once\n",
      "Currently exploring poem  339\n",
      "Run once\n",
      "Currently exploring poem  340\n",
      "Run once\n",
      "Currently exploring poem  341\n",
      "Run once\n",
      "Currently exploring poem  342\n",
      "Run once\n",
      "Currently exploring poem  343\n",
      "Run once\n",
      "Currently exploring poem  344\n",
      "Run once\n",
      "Currently exploring poem  345\n",
      "Run once\n",
      "Currently exploring poem  346\n",
      "Run once\n",
      "Currently exploring poem  347\n",
      "Run once\n",
      "Currently exploring poem  348\n",
      "Run once\n",
      "Currently exploring poem  349\n",
      "Run once\n",
      "Currently exploring poem  350\n",
      "Run once\n",
      "Currently exploring poem  351\n",
      "Run once\n",
      "Currently exploring poem  352\n",
      "Run once\n",
      "Currently exploring poem  353\n",
      "Run once\n",
      "Currently exploring poem  354\n",
      "Run once\n",
      "Currently exploring poem  355\n",
      "Run once\n",
      "Currently exploring poem  356\n",
      "Run once\n",
      "Currently exploring poem  357\n",
      "Run once\n",
      "Currently exploring poem  358\n",
      "Run once\n",
      "Currently exploring poem  359\n",
      "Run once\n",
      "Currently exploring poem  360\n",
      "Run once\n",
      "Currently exploring poem  361\n",
      "Run once\n",
      "Currently exploring poem  362\n",
      "Run once\n",
      "Currently exploring poem  363\n",
      "Run once\n",
      "Currently exploring poem  364\n",
      "Run once\n",
      "Currently exploring poem  365\n",
      "Run once\n",
      "Currently exploring poem  366\n",
      "Run once\n",
      "Currently exploring poem  367\n",
      "Run once\n",
      "Currently exploring poem  368\n",
      "Run once\n",
      "Currently exploring poem  369\n",
      "Run once\n",
      "Currently exploring poem  370\n",
      "Run once\n",
      "Currently exploring poem  371\n",
      "Run once\n",
      "Currently exploring poem  372\n",
      "Run once\n",
      "Currently exploring poem  373\n",
      "Run once\n",
      "Currently exploring poem  374\n",
      "Run once\n",
      "Currently exploring poem  375\n",
      "Run once\n",
      "Currently exploring poem  376\n",
      "Run once\n",
      "Currently exploring poem  377\n",
      "Run once\n",
      "Currently exploring poem  378\n",
      "Run once\n",
      "Currently exploring poem  379\n",
      "Run once\n",
      "Currently exploring poem  380\n",
      "Run once\n",
      "Currently exploring poem  381\n",
      "Run once\n",
      "Currently exploring poem  382\n",
      "Run once\n",
      "Currently exploring poem  383\n",
      "Run once\n",
      "Currently exploring poem  384\n",
      "Run once\n",
      "Currently exploring poem  385\n",
      "Run once\n",
      "Currently exploring poem  386\n",
      "Run once\n",
      "Currently exploring poem  387\n",
      "Run once\n",
      "Currently exploring poem  388\n",
      "Run once\n",
      "Currently exploring poem  389\n",
      "Run once\n",
      "Currently exploring poem  390\n",
      "Run once\n",
      "Total Number of Documents: 391\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Preprocess text (tokenization and removing stopwords)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    texts = []\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(['from', 'like', 'thou', 'may', 'much','let','ye','said','yarrow','tis','thy','whose','thee','yet','shall','one', 'see','every','amp','even','juan','upon','though','oh'])\n",
    "\n",
    "\n",
    "    \n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in stop]\n",
    "        texts.append(stopped_tokens)\n",
    "\n",
    "\n",
    "\n",
    "    return texts\n",
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Conversion into a document-term matrix for feeding into LSIModel for SVD reduction\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    return dictionary,doc_term_matrix\n",
    "\n",
    "document_list =load_data(poemSet)\n",
    "clean_text=preprocess_data(document_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we'll build our LSA model and calculate the CoherenceModel score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Matrix:\n",
      "[(0,\n",
      "  '0.151*\"still\" + 0.145*\"eyes\" + 0.145*\"light\" + 0.141*\"day\" + 0.138*\"love\" + '\n",
      "  '0.135*\"heart\" + 0.120*\"night\" + 0.114*\"oer\" + 0.108*\"sweet\" + 0.100*\"came\" '\n",
      "  '+ 0.098*\"would\" + 0.098*\"life\" + 0.096*\"old\" + 0.096*\"made\" + 0.093*\"earth\" '\n",
      "  '+ 0.093*\"dark\" + 0.090*\"death\" + 0.090*\"bright\" + 0.090*\"world\" + '\n",
      "  '0.087*\"thus\"'),\n",
      " (1,\n",
      "  '-0.348*\"lady\" + -0.310*\"christabel\" + -0.197*\"geraldine\" + -0.169*\"leoline\" '\n",
      "  '+ -0.164*\"sir\" + -0.124*\"maid\" + 0.113*\"dark\" + -0.109*\"sweet\" + '\n",
      "  '-0.105*\"well\" + -0.099*\"hath\" + -0.096*\"ladys\" + -0.088*\"saw\" + '\n",
      "  '0.087*\"earth\" + -0.080*\"look\" + -0.080*\"eyes\" + 0.079*\"death\" + '\n",
      "  '-0.076*\"child\" + -0.076*\"say\" + -0.076*\"tell\" + 0.075*\"ever\"'),\n",
      " (2,\n",
      "  '0.173*\"dark\" + 0.172*\"eyes\" + -0.161*\"time\" + -0.153*\"seen\" + '\n",
      "  '0.126*\"christabel\" + -0.125*\"man\" + -0.115*\"could\" + 0.111*\"lady\" + '\n",
      "  '0.096*\"bright\" + 0.094*\"sleep\" + -0.090*\"lie\" + 0.088*\"sweet\" + '\n",
      "  '-0.086*\"know\" + 0.080*\"geraldine\" + 0.080*\"fled\" + -0.078*\"long\" + '\n",
      "  '-0.075*\"say\" + -0.073*\"little\" + -0.073*\"without\" + -0.073*\"must\"'),\n",
      " (3,\n",
      "  '-0.168*\"light\" + 0.149*\"dark\" + 0.135*\"seen\" + -0.106*\"came\" + '\n",
      "  '-0.105*\"dead\" + -0.103*\"sun\" + -0.103*\"adonais\" + 0.087*\"ever\" + '\n",
      "  '-0.085*\"tears\" + 0.084*\"well\" + 0.083*\"little\" + -0.080*\"weep\" + '\n",
      "  '0.080*\"still\" + 0.078*\"boat\" + 0.077*\"human\" + 0.077*\"poet\" + '\n",
      "  '-0.077*\"shadows\" + -0.076*\"dew\" + 0.075*\"eyes\" + -0.074*\"cold\"'),\n",
      " (4,\n",
      "  '-0.219*\"mind\" + -0.159*\"nature\" + -0.135*\"joy\" + -0.118*\"among\" + '\n",
      "  '-0.117*\"things\" + 0.110*\"light\" + -0.105*\"thus\" + -0.105*\"hills\" + '\n",
      "  '-0.102*\"soul\" + -0.088*\"hence\" + 0.086*\"death\" + -0.085*\"power\" + '\n",
      "  '0.081*\"world\" + -0.080*\"eye\" + -0.077*\"still\" + -0.076*\"objects\" + '\n",
      "  '-0.074*\"first\" + 0.073*\"night\" + 0.072*\"eyes\" + 0.071*\"others\"'),\n",
      " (5,\n",
      "  '0.157*\"death\" + -0.147*\"sun\" + -0.132*\"seemed\" + -0.127*\"moved\" + '\n",
      "  '0.122*\"heart\" + 0.121*\"adonais\" + 0.120*\"love\" + -0.116*\"shape\" + '\n",
      "  '-0.114*\"shadows\" + 0.106*\"tears\" + -0.103*\"old\" + -0.098*\"sea\" + '\n",
      "  '-0.096*\"within\" + -0.092*\"fell\" + 0.088*\"poor\" + 0.084*\"pale\" + '\n",
      "  '-0.083*\"ere\" + -0.081*\"world\" + -0.080*\"others\" + 0.079*\"grief\"'),\n",
      " (6,\n",
      "  '-0.158*\"ship\" + -0.143*\"came\" + -0.132*\"sea\" + 0.123*\"mind\" + -0.114*\"wide\" '\n",
      "  '+ -0.114*\"heard\" + 0.112*\"world\" + 0.110*\"light\" + -0.110*\"mist\" + '\n",
      "  '-0.101*\"agnes\" + -0.099*\"never\" + -0.091*\"hand\" + -0.090*\"still\" + '\n",
      "  '0.088*\"christabel\" + -0.087*\"poor\" + -0.087*\"porphyro\" + -0.085*\"body\" + '\n",
      "  '-0.084*\"mariner\" + -0.084*\"moon\" + -0.083*\"sun\"'),\n",
      " (7,\n",
      "  '-0.181*\"poor\" + 0.127*\"came\" + -0.116*\"oer\" + -0.113*\"rude\" + 0.113*\"death\" '\n",
      "  '+ 0.104*\"sea\" + 0.100*\"dead\" + -0.099*\"old\" + 0.096*\"adonais\" + '\n",
      "  '-0.096*\"care\" + -0.095*\"new\" + -0.086*\"age\" + 0.084*\"spirit\" + '\n",
      "  '-0.084*\"vain\" + -0.083*\"amid\" + 0.080*\"ship\" + -0.078*\"till\" + 0.074*\"mist\" '\n",
      "  '+ -0.074*\"joys\" + -0.072*\"shadows\"'),\n",
      " (8,\n",
      "  '0.163*\"agnes\" + -0.154*\"oer\" + 0.149*\"old\" + 0.145*\"st\" + 0.140*\"porphyro\" '\n",
      "  '+ 0.116*\"madeline\" + 0.106*\"eyes\" + -0.104*\"till\" + -0.101*\"sea\" + '\n",
      "  '0.092*\"soft\" + 0.092*\"wide\" + -0.090*\"sky\" + 0.089*\"saturn\" + -0.086*\"rude\" '\n",
      "  '+ -0.083*\"wild\" + -0.082*\"death\" + 0.080*\"silver\" + -0.080*\"ship\" + '\n",
      "  '-0.079*\"song\" + -0.078*\"poor\"'),\n",
      " (9,\n",
      "  '-0.452*\"wi\" + -0.382*\"tam\" + -0.125*\"auld\" + -0.120*\"night\" + -0.118*\"thro\" '\n",
      "  '+ -0.117*\"whare\" + -0.116*\"ae\" + -0.102*\"storm\" + -0.100*\"frae\" + '\n",
      "  '-0.096*\"na\" + -0.096*\"mony\" + -0.094*\"wad\" + -0.092*\"maggie\" + '\n",
      "  '-0.086*\"lang\" + -0.082*\"till\" + -0.072*\"bonie\" + -0.067*\"sae\" + -0.065*\"ah\" '\n",
      "  '+ -0.062*\"mind\" + -0.062*\"near\"'),\n",
      " (10,\n",
      "  '-0.165*\"saturn\" + 0.149*\"love\" + 0.124*\"agnes\" + 0.120*\"sweet\" + '\n",
      "  '-0.113*\"poor\" + 0.109*\"st\" + 0.109*\"joy\" + -0.109*\"still\" + '\n",
      "  '0.106*\"porphyro\" + -0.103*\"sad\" + 0.098*\"rude\" + 0.096*\"heart\" + '\n",
      "  '-0.090*\"thea\" + 0.089*\"madeline\" + -0.088*\"thus\" + -0.083*\"cannot\" + '\n",
      "  '-0.077*\"space\" + -0.072*\"sorrow\" + -0.071*\"gods\" + -0.070*\"solemn\"'),\n",
      " (11,\n",
      "  '-0.140*\"saturn\" + -0.114*\"rude\" + 0.111*\"man\" + -0.103*\"new\" + '\n",
      "  '-0.099*\"joys\" + -0.093*\"joy\" + 0.089*\"mind\" + -0.089*\"earth\" + '\n",
      "  '-0.084*\"voice\" + 0.083*\"poor\" + 0.082*\"hand\" + -0.078*\"meadows\" + '\n",
      "  '-0.077*\"come\" + -0.076*\"might\" + -0.076*\"wi\" + -0.075*\"thea\" + '\n",
      "  '-0.074*\"gods\" + 0.069*\"eyes\" + -0.069*\"sky\" + -0.068*\"ear\"'),\n",
      " (12,\n",
      "  '-0.201*\"could\" + -0.201*\"misery\" + -0.184*\"thorn\" + -0.155*\"little\" + '\n",
      "  '-0.151*\"moss\" + -0.118*\"day\" + -0.116*\"love\" + -0.114*\"pond\" + '\n",
      "  '-0.114*\"hill\" + -0.108*\"would\" + -0.100*\"mountain\" + 0.093*\"still\" + '\n",
      "  '-0.093*\"saw\" + -0.090*\"infants\" + -0.089*\"heard\" + -0.089*\"dungeon\" + '\n",
      "  '-0.088*\"grave\" + -0.087*\"last\" + -0.080*\"know\" + -0.080*\"never\"')]\n",
      "\n",
      "Coherence Score:  0.2987748975672979\n"
     ]
    }
   ],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    '''\n",
    "    Use SVD on the document term matrix and output the LSA model topics\n",
    "    as well as coherence calculated with CoherenceModel\n",
    "    '''\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    print('Document Term Matrix:')\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary, power_iters=50, onepass=False)  # train model\n",
    "    pprint(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    coherencemodel = CoherenceModel(model=lsamodel, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherencemodel.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return lsamodel\n",
    "\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
