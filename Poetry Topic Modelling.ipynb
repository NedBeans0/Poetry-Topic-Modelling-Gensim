{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Poetry Topic Modelling with Latent Dirichlet Allocation and Latent Semantic Analysis</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was utilised as part of my final year undergraduate project and dissertation, whose abstract is as follows:\n",
    "\n",
    "\n",
    "<b>Topic models, which detect latent themes in a corpus of documents to group co-occurring keywords together in thematically comprehensible ways, were generated using the Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) algorithms with three datasets of poetry from different time periods. A close reading of the results as well as a study to measure interpretability were used to measure which algorithm was the most successful at uncovering specific themes in each dataset established using relevant literary studies. Comparison between the two algorithms’ performances served to indicate which method was the most successful in modelling this highly figurative language. Our findings indicated that LDA generated the most thematically comprehensible topics, owing to improved performance in identifying context and polysemy in the vocabulary used throughout the corpora, as well as having more parameters available to tune and optimise performance. <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps taken to build our models in this notebook:\n",
    "1. Mine a large CSV file of poems from Kaggle to create smaller CSV files for each of the poetic movements we are exploring (Romantic, Metaphyiscal, Harlem Renaissance)\n",
    "\n",
    "2. Clean and preprocess the data, creating bigrams, dictionaries and document-term matrices ready to be passed into the Gensim model functions\n",
    "\n",
    "3. Evaluation and validation of topics generated. \n",
    "\n",
    "Step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports for mining the Kaggle CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the original Kaggle dataset into a Pandas dataframe and making empty dataframes for each poetic movement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poetrydata = pd.read_csv('kaggle_poem_dataset.csv') #This is a csv containing many PoetryFoundation poems\n",
    "\n",
    "metaphysical = pd.DataFrame()\n",
    "romantic = pd.DataFrame()\n",
    "harlem = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function <b>addPoems</b> takes the main CSV and the name of a poet as input, and adds any poems from that poet into the dataframe we're building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPoems(poetrydata, poetname):\n",
    "    #Add every row (poem) whose author is the poet we specify into an object \n",
    "    newPoems = poetrydata[poetrydata['Author'].str.contains(poetname)]\n",
    "    return newPoems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lists for each poet whose works we want to explore are created for each movement. Identifying the poets whose work we would want to explore was done by searching for the poets in the Romantic and Harlem Renaissance movements as listed on poetryfoundation.org. Identifying the Metaphysical poets (a harder task) was done through researching several sources detailed further in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaphysicalPoets = ['John Donne', 'Abraham Cowley', 'Andrew Marvell', 'Richard Crashaw','George Herbert','John Cleveland', 'Henry Vaughan']\n",
    "romanticPoets = ['George Gordon','William Blake','Shelley','Felicia Dorothea Hemans','William Wordsworth','Coleridge','Keats','John Clare','Beddoes','William Lisle Bowles','Robert Burns','Barbauld','Heinrich Heine','Friedrich Hölderlin','Charles Lamb','Thomas Moore','Giacamo Leopardi','Christian Milne','Walter Scott','Robert Southey','Mary Lamb','Elizabeth Moody','Anna Seward','Elizabeth Bentley','Helen Leigh','George Crabbe','Joanna Baillie','Letitia Elizabeth Landon','Helen Maria Williams','Matilda Bethem','Mary Robinson','Walter Savage Landor','Leigh Hunt','Charlotte Smith','John Clare','Thomas Hood','Elizabeth Hands','Dorothy Wordsworth','Charlotte Richardson','Jane Taylor','Hartley Coleridge']\n",
    "harlemPoets = ['Langston Hughes','Paul Dunbar','Claude McKay','Melvin B. Tolson','James Weldon Johnson','Fenton Johnson','Countee Cullen','Anne Spencer','William Warning Cuney','Margaret Walker','Jean Toomer','Georgia Douglas Johnson','W. E. B. Du Bois','Arna Bontemps','Leslie Pickeny Hill','Sterling A. Brown','Alice Dunbar-Nelson','Jessie Redmon Fauset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using for loops and our <b>addPoems</b> function to build our DataFrames, then verifying their sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaphysical Poems: \n",
      " 126\n",
      "Harlem Renaissance Poems: \n",
      " 82\n",
      "Romantic Poems: \n",
      " 392\n"
     ]
    }
   ],
   "source": [
    "#Metaphysical\n",
    "for poet in metaphysicalPoets:\n",
    "    newPoems = addPoems(poetrydata, poet) \n",
    "    metaphysical = metaphysical.append(newPoems, ignore_index=True)\n",
    "\n",
    "#Romantic\n",
    "for poet in romanticPoets:\n",
    "    newPoems = addPoems(poetrydata, poet) \n",
    "    romantic = romantic.append(newPoems, ignore_index=True)\n",
    "\n",
    "#Harlem Renaissance\n",
    "for poet in harlemPoets:\n",
    "    newPoems = addPoems(poetrydata, poet) \n",
    "    harlem = harlem.append(newPoems, ignore_index=True)\n",
    "    \n",
    "    \n",
    "print('Metaphysical Poems: \\n', metaphysical.shape[0]) #shape[0] = row count = amount of poems\n",
    "print('Harlem Renaissance Poems: \\n', harlem.shape[0])\n",
    "print('Romantic Poems: \\n', romantic.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function createCSV takes the movement DataFrame and the name of the movement as parameters and is used to produce a CSV file for each movement's poems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCSV(movementDF, movementName):\n",
    "    content = pd.DataFrame(movementDF, columns=['Content'])#We only want the Content column (the poems)\n",
    "    content = content.replace('\\n',' ', regex=True) #Remove line breaks for formatting\n",
    "    export_csv = content.to_csv(r\"\"+movementName+\".csv\", index = None, header=True) \n",
    "\n",
    "createCSV(romantic, 'romantic')\n",
    "createCSV(metaphysical, 'metaphysical')\n",
    "createCSV(harlem,'harlem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our datasets prepared, we will create the LDA and LSA models.\n",
    "\n",
    "Step 2:\n",
    "First we'll import the necessary libraries, making it clear which modules we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib \n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora, models, utils\n",
    "from gensim import similarities\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "import string\n",
    "import re #regex\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LsiModel\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll specify the movement we wish to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify the poetry movement (romantic, metaphysical, harlem)\n",
      "romantic\n"
     ]
    }
   ],
   "source": [
    "poemSet = input('Please specify the poetry movement (romantic, metaphysical, harlem)\\n')\n",
    "poems = pd.read_csv(poemSet+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will preprocess the data ready for bigram models and a document-term matrix to be built from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(poems):\n",
    "    for poem in poems:\n",
    "        yield(gensim.utils.simple_preprocess(str(poem))) #For formatting \n",
    "\n",
    "\n",
    "#Remove punctuation\n",
    "poems[\"poems_processed\"] = poems['Content'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#Make all lowercase\n",
    "poems[\"poems_processed\"] = poems['poems_processed'].str.lower()\n",
    "\n",
    "#Remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "#Many of our poems contain some antiquated language not accounted for in NLTK's stopwords collection, so we need \n",
    "#to add them. A few other words have been added which consistently made their way into almost every topic and needed\n",
    "#to be processed out (these words are not very substantive anyway)\n",
    "stop.extend(['from', 'like', 'thou', 'may', 'much','let','ye','said','tis','thy','whose','thee','yet','shall','one', 'see','every','amp','even','juan','yarrow','upon','though','oh'])\n",
    "poems['poems_processed'] = poems['poems_processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#Put column of poems into new variable\n",
    "poemtemp = poems['poems_processed']\n",
    "\n",
    "#Conversion of the processed column into its own dataframe then a list to keep formatting\n",
    "datadf = poemtemp.to_frame() \n",
    "data = datadf['poems_processed'].values.tolist()\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions we'll use to format and create bigrams from our corpus respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams, a dictionary, preparing the corpus and finally a document-term matrix to pass into the model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the LDA model using MALLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.024*\"love\" + 0.015*\"light\" + 0.014*\"sleep\" + 0.013*\"death\" + '\n",
      "  '0.013*\"flowers\" + 0.012*\"tears\" + 0.011*\"cold\" + 0.011*\"smile\" + '\n",
      "  '0.011*\"voice\" + 0.010*\"thine\" + 0.009*\"pale\" + 0.009*\"wings\" + '\n",
      "  '0.009*\"bright\" + 0.009*\"till\" + 0.009*\"woe\" + 0.009*\"dead\" + 0.009*\"weep\" + '\n",
      "  '0.009*\"golden\" + 0.009*\"wild\" + 0.008*\"cloud\"'),\n",
      " (1,\n",
      "  '0.018*\"green\" + 0.016*\"hear\" + 0.012*\"sun\" + 0.011*\"spring\" + 0.011*\"make\" '\n",
      "  '+ 0.010*\"die\" + 0.009*\"water\" + 0.009*\"birds\" + 0.009*\"fancy\" + '\n",
      "  '0.009*\"home\" + 0.009*\"bird\" + 0.008*\"mine\" + 0.008*\"high\" + 0.008*\"quiet\" + '\n",
      "  '0.008*\"happy\" + 0.008*\"bring\" + 0.008*\"round\" + 0.007*\"oer\" + 0.007*\"clear\" '\n",
      "  '+ 0.007*\"lake\"'),\n",
      " (2,\n",
      "  '0.017*\"eyes\" + 0.015*\"sweet\" + 0.012*\"lady\" + 0.011*\"lay\" + 0.011*\"side\" + '\n",
      "  '0.010*\"heard\" + 0.010*\"made\" + 0.010*\"heart\" + 0.010*\"stood\" + 0.010*\"fair\" '\n",
      "  '+ 0.009*\"white\" + 0.009*\"face\" + 0.009*\"hath\" + 0.009*\"bright\" + '\n",
      "  '0.008*\"arms\" + 0.007*\"child\" + 0.007*\"eye\" + 0.007*\"full\" + 0.007*\"rose\" + '\n",
      "  '0.007*\"hour\"'),\n",
      " (3,\n",
      "  '0.017*\"world\" + 0.012*\"light\" + 0.011*\"air\" + 0.011*\"heaven\" + 0.010*\"soft\" '\n",
      "  '+ 0.009*\"high\" + 0.009*\"waves\" + 0.009*\"form\" + 0.008*\"fire\" + 0.008*\"lost\" '\n",
      "  '+ 0.007*\"clouds\" + 0.007*\"deep\" + 0.007*\"divine\" + 0.006*\"faint\" + '\n",
      "  '0.006*\"oer\" + 0.006*\"feet\" + 0.006*\"fell\" + 0.006*\"god\" + 0.006*\"amid\" + '\n",
      "  '0.006*\"shadows\"'),\n",
      " (4,\n",
      "  '0.020*\"dark\" + 0.018*\"sweet\" + 0.016*\"sky\" + 0.013*\"stream\" + 0.013*\"life\" '\n",
      "  '+ 0.011*\"silent\" + 0.011*\"leaves\" + 0.010*\"voice\" + 0.009*\"moon\" + '\n",
      "  '0.009*\"hills\" + 0.008*\"shade\" + 0.008*\"sound\" + 0.008*\"woods\" + '\n",
      "  '0.007*\"spread\" + 0.007*\"solemn\" + 0.007*\"mid\" + 0.006*\"wind\" + '\n",
      "  '0.006*\"forms\" + 0.006*\"sleep\" + 0.006*\"living\"'),\n",
      " (5,\n",
      "  '0.022*\"mind\" + 0.017*\"nature\" + 0.013*\"long\" + 0.012*\"eye\" + 0.012*\"feel\" + '\n",
      "  '0.012*\"sense\" + 0.012*\"joy\" + 0.009*\"oer\" + 0.008*\"pain\" + 0.008*\"pleasure\" '\n",
      "  '+ 0.007*\"vain\" + 0.007*\"things\" + 0.006*\"man\" + 0.006*\"hour\" + 0.006*\"time\" '\n",
      "  '+ 0.006*\"race\" + 0.006*\"reason\" + 0.006*\"fate\" + 0.005*\"mood\" + '\n",
      "  '0.005*\"oft\"'),\n",
      " (6,\n",
      "  '0.030*\"love\" + 0.025*\"heart\" + 0.017*\"life\" + 0.014*\"years\" + 0.014*\"power\" '\n",
      "  '+ 0.013*\"soul\" + 0.013*\"art\" + 0.012*\"day\" + 0.011*\"doth\" + 0.011*\"time\" + '\n",
      "  '0.011*\"past\" + 0.011*\"dear\" + 0.011*\"human\" + 0.010*\"delight\" + '\n",
      "  '0.010*\"thoughts\" + 0.010*\"hath\" + 0.009*\"spirit\" + 0.009*\"thought\" + '\n",
      "  '0.009*\"live\" + 0.009*\"give\"'),\n",
      " (7,\n",
      "  '0.018*\"meet\" + 0.018*\"wi\" + 0.013*\"ill\" + 0.012*\"night\" + 0.011*\"dear\" + '\n",
      "  '0.011*\"sweet\" + 0.010*\"thro\" + 0.010*\"till\" + 0.009*\"body\" + 0.007*\"sae\" + '\n",
      "  '0.007*\"neer\" + 0.007*\"bonie\" + 0.006*\"tam\" + 0.006*\"poor\" + 0.005*\"frae\" + '\n",
      "  '0.005*\"auld\" + 0.005*\"wooin_ot\" + 0.005*\"mary\" + 0.005*\"john\" + '\n",
      "  '0.005*\"tho\"'),\n",
      " (8,\n",
      "  '0.017*\"hand\" + 0.012*\"land\" + 0.011*\"man\" + 0.011*\"men\" + 0.011*\"door\" + '\n",
      "  '0.009*\"poor\" + 0.009*\"rich\" + 0.009*\"long\" + 0.009*\"left\" + 0.009*\"care\" + '\n",
      "  '0.008*\"red\" + 0.007*\"save\" + 0.006*\"laid\" + 0.006*\"fly\" + 0.006*\"sat\" + '\n",
      "  '0.006*\"hold\" + 0.006*\"sigh\" + 0.006*\"deep\" + 0.005*\"tower\" + 0.005*\"house\"'),\n",
      " (9,\n",
      "  '0.012*\"oer\" + 0.011*\"joy\" + 0.009*\"round\" + 0.009*\"morn\" + 0.008*\"song\" + '\n",
      "  '0.008*\"wild\" + 0.007*\"rude\" + 0.007*\"sound\" + 0.007*\"sit\" + 0.006*\"sweet\" + '\n",
      "  '0.006*\"joys\" + 0.006*\"beauty\" + 0.006*\"sunny\" + 0.005*\"muse\" + '\n",
      "  '0.005*\"sudden\" + 0.005*\"health\" + 0.005*\"long\" + 0.005*\"heart\" + '\n",
      "  '0.005*\"early\" + 0.005*\"leaves\"'),\n",
      " (10,\n",
      "  '0.015*\"fair\" + 0.014*\"day\" + 0.011*\"true\" + 0.011*\"face\" + 0.010*\"love\" + '\n",
      "  '0.010*\"breast\" + 0.010*\"child\" + 0.008*\"warm\" + 0.008*\"poor\" + '\n",
      "  '0.008*\"heard\" + 0.008*\"full\" + 0.008*\"head\" + 0.008*\"misery\" + '\n",
      "  '0.008*\"mothers\" + 0.008*\"father\" + 0.007*\"wind\" + 0.007*\"hill\" + '\n",
      "  '0.007*\"children\" + 0.007*\"fond\" + 0.007*\"grave\"'),\n",
      " (11,\n",
      "  '0.020*\"night\" + 0.016*\"sea\" + 0.014*\"earth\" + 0.013*\"day\" + 0.011*\"sun\" + '\n",
      "  '0.011*\"light\" + 0.009*\"calm\" + 0.009*\"thought\" + 0.009*\"things\" + '\n",
      "  '0.008*\"dream\" + 0.008*\"deep\" + 0.008*\"spirit\" + 0.008*\"mountains\" + '\n",
      "  '0.007*\"ocean\" + 0.007*\"winds\" + 0.007*\"soul\" + 0.007*\"round\" + '\n",
      "  '0.007*\"beneath\" + 0.007*\"green\" + 0.006*\"till\"'),\n",
      " (12,\n",
      "  '0.012*\"man\" + 0.011*\"great\" + 0.010*\"good\" + 0.008*\"time\" + 0.008*\"young\" + '\n",
      "  '0.007*\"turn\" + 0.007*\"ive\" + 0.007*\"place\" + 0.007*\"ill\" + 0.006*\"late\" + '\n",
      "  '0.006*\"king\" + 0.005*\"find\" + 0.005*\"truth\" + 0.005*\"word\" + 0.005*\"made\" + '\n",
      "  '0.005*\"thing\" + 0.005*\"show\" + 0.005*\"god\" + 0.005*\"ere\" + 0.005*\"twas\"')]\n"
     ]
    }
   ],
   "source": [
    "mallet_path = '/Users/admin/mallet-2.0.8/bin/mallet' #path to mallet\n",
    "\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=13, id2word=id2word,\n",
    "                                                random_seed=100, iterations=500) #Increase iterations for improvement\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(num_topics=-1, num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll compute the Coherence score as a quantitative performance metric (as detailed in the report, this does not really tell us much in this project and seems essentially arbitrary, but will be included for demonstration purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3601080630415803\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_words_bigrams, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll perform LSA, by first taking as input the name of the poetic movement to be explored (we will assume for the purposes of this notebook's usability that it will be the same as the one specified previously for LDA) and setting up some other variables. LSA won't work as intuitively using DataFrames so we'll prepare the poems in a list instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \n",
    "    documents_list = []\n",
    "    titles=[]\n",
    "    poems = pd.read_csv(file_name+'.csv')\n",
    "    poemnum = poems.shape[0]\n",
    "    print(\"Poemnum = \",poemnum)\n",
    "\n",
    "    \n",
    "    for i in range(poemnum-1):\n",
    "        forname = str(i)\n",
    "        file = open(poemSet+\"Txt\"+ \"/Poem\" +forname+\".txt\", \"r\") \n",
    "        #print(\"Currently exploring poem \" , i )\n",
    "        for line in file.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "\n",
    "        file.close() \n",
    "        #print('Run once')\n",
    "\n",
    "    print(\"Total Number of Documents:\",len(documents_list))\n",
    "    #titles.append( text[0:min(len(text),100)] )\n",
    "    return documents_list\n",
    "\n",
    "# LSA Model\n",
    "number_of_topics=13\n",
    "words=20\n",
    "#poemSetLSA = poemSet as defined earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll preprocess this new list in the same way, with the same stopwords in order to convert into a new document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poemnum =  392\n",
      "Total Number of Documents: 391\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Preprocess text (tokenization and removing stopwords)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    texts = []\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(['from', 'like', 'thou', 'may', 'much','let','ye','said','yarrow','tis','thy','whose','thee','yet','shall','one', 'see','every','amp','even','juan','upon','though','oh'])\n",
    "\n",
    "\n",
    "    \n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in stop]\n",
    "        texts.append(stopped_tokens)\n",
    "\n",
    "\n",
    "\n",
    "    return texts\n",
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Conversion into a document-term matrix for feeding into LSIModel for SVD reduction\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    return dictionary,doc_term_matrix\n",
    "\n",
    "document_list =load_data(poemSet)\n",
    "clean_text=preprocess_data(document_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we'll build our LSA model and calculate the CoherenceModel score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Term Matrix:\n",
      "[(0,\n",
      "  '0.151*\"still\" + 0.145*\"eyes\" + 0.145*\"light\" + 0.141*\"day\" + 0.138*\"love\" + '\n",
      "  '0.135*\"heart\" + 0.120*\"night\" + 0.114*\"oer\" + 0.108*\"sweet\" + 0.100*\"came\" '\n",
      "  '+ 0.098*\"would\" + 0.098*\"life\" + 0.096*\"old\" + 0.096*\"made\" + 0.093*\"earth\" '\n",
      "  '+ 0.093*\"dark\" + 0.090*\"death\" + 0.090*\"bright\" + 0.090*\"world\" + '\n",
      "  '0.087*\"thus\"'),\n",
      " (1,\n",
      "  '-0.348*\"lady\" + -0.310*\"christabel\" + -0.197*\"geraldine\" + -0.169*\"leoline\" '\n",
      "  '+ -0.164*\"sir\" + -0.124*\"maid\" + 0.113*\"dark\" + -0.109*\"sweet\" + '\n",
      "  '-0.105*\"well\" + -0.099*\"hath\" + -0.096*\"ladys\" + -0.088*\"saw\" + '\n",
      "  '0.087*\"earth\" + -0.080*\"look\" + -0.080*\"eyes\" + 0.079*\"death\" + '\n",
      "  '-0.076*\"child\" + -0.076*\"say\" + -0.076*\"tell\" + 0.075*\"ever\"'),\n",
      " (2,\n",
      "  '0.173*\"dark\" + 0.172*\"eyes\" + -0.161*\"time\" + -0.153*\"seen\" + '\n",
      "  '0.126*\"christabel\" + -0.125*\"man\" + -0.115*\"could\" + 0.111*\"lady\" + '\n",
      "  '0.096*\"bright\" + 0.094*\"sleep\" + -0.090*\"lie\" + 0.088*\"sweet\" + '\n",
      "  '-0.086*\"know\" + 0.080*\"geraldine\" + 0.080*\"fled\" + -0.078*\"long\" + '\n",
      "  '-0.075*\"say\" + -0.073*\"little\" + -0.073*\"without\" + -0.073*\"must\"'),\n",
      " (3,\n",
      "  '0.168*\"light\" + -0.149*\"dark\" + -0.135*\"seen\" + 0.106*\"came\" + 0.105*\"dead\" '\n",
      "  '+ 0.103*\"sun\" + 0.103*\"adonais\" + -0.087*\"ever\" + 0.085*\"tears\" + '\n",
      "  '-0.084*\"well\" + -0.083*\"little\" + 0.080*\"weep\" + -0.080*\"still\" + '\n",
      "  '-0.078*\"boat\" + -0.077*\"human\" + -0.077*\"poet\" + 0.077*\"shadows\" + '\n",
      "  '0.076*\"dew\" + -0.075*\"eyes\" + 0.074*\"cold\"'),\n",
      " (4,\n",
      "  '-0.219*\"mind\" + -0.159*\"nature\" + -0.135*\"joy\" + -0.118*\"among\" + '\n",
      "  '-0.117*\"things\" + 0.110*\"light\" + -0.105*\"thus\" + -0.105*\"hills\" + '\n",
      "  '-0.102*\"soul\" + -0.088*\"hence\" + 0.086*\"death\" + -0.085*\"power\" + '\n",
      "  '0.081*\"world\" + -0.080*\"eye\" + -0.077*\"still\" + -0.076*\"objects\" + '\n",
      "  '-0.074*\"first\" + 0.073*\"night\" + 0.072*\"eyes\" + 0.071*\"others\"'),\n",
      " (5,\n",
      "  '-0.157*\"death\" + 0.147*\"sun\" + 0.132*\"seemed\" + 0.127*\"moved\" + '\n",
      "  '-0.122*\"heart\" + -0.121*\"adonais\" + -0.120*\"love\" + 0.116*\"shape\" + '\n",
      "  '0.114*\"shadows\" + -0.106*\"tears\" + 0.103*\"old\" + 0.098*\"sea\" + '\n",
      "  '0.096*\"within\" + 0.092*\"fell\" + -0.088*\"poor\" + -0.084*\"pale\" + 0.083*\"ere\" '\n",
      "  '+ 0.081*\"world\" + 0.080*\"others\" + -0.079*\"grief\"'),\n",
      " (6,\n",
      "  '0.158*\"ship\" + 0.143*\"came\" + 0.132*\"sea\" + -0.123*\"mind\" + 0.114*\"wide\" + '\n",
      "  '0.114*\"heard\" + -0.112*\"world\" + -0.110*\"light\" + 0.110*\"mist\" + '\n",
      "  '0.101*\"agnes\" + 0.099*\"never\" + 0.091*\"hand\" + 0.090*\"still\" + '\n",
      "  '-0.088*\"christabel\" + 0.087*\"poor\" + 0.087*\"porphyro\" + 0.085*\"body\" + '\n",
      "  '0.084*\"mariner\" + 0.084*\"moon\" + 0.083*\"sun\"'),\n",
      " (7,\n",
      "  '0.181*\"poor\" + -0.127*\"came\" + 0.116*\"oer\" + 0.113*\"rude\" + -0.113*\"death\" '\n",
      "  '+ -0.104*\"sea\" + -0.100*\"dead\" + 0.099*\"old\" + -0.096*\"adonais\" + '\n",
      "  '0.096*\"care\" + 0.095*\"new\" + 0.086*\"age\" + -0.084*\"spirit\" + 0.084*\"vain\" + '\n",
      "  '0.083*\"amid\" + -0.080*\"ship\" + 0.078*\"till\" + -0.074*\"mist\" + 0.074*\"joys\" '\n",
      "  '+ 0.072*\"shadows\"'),\n",
      " (8,\n",
      "  '-0.163*\"agnes\" + 0.154*\"oer\" + -0.149*\"old\" + -0.145*\"st\" + '\n",
      "  '-0.140*\"porphyro\" + -0.116*\"madeline\" + -0.106*\"eyes\" + 0.104*\"till\" + '\n",
      "  '0.101*\"sea\" + -0.092*\"soft\" + -0.092*\"wide\" + 0.090*\"sky\" + -0.089*\"saturn\" '\n",
      "  '+ 0.086*\"rude\" + 0.083*\"wild\" + 0.082*\"death\" + -0.080*\"silver\" + '\n",
      "  '0.080*\"ship\" + 0.079*\"song\" + 0.078*\"poor\"'),\n",
      " (9,\n",
      "  '0.452*\"wi\" + 0.382*\"tam\" + 0.125*\"auld\" + 0.120*\"night\" + 0.118*\"thro\" + '\n",
      "  '0.117*\"whare\" + 0.116*\"ae\" + 0.102*\"storm\" + 0.100*\"frae\" + 0.096*\"na\" + '\n",
      "  '0.096*\"mony\" + 0.094*\"wad\" + 0.092*\"maggie\" + 0.086*\"lang\" + 0.082*\"till\" + '\n",
      "  '0.072*\"bonie\" + 0.067*\"sae\" + 0.065*\"ah\" + 0.062*\"mind\" + 0.062*\"near\"'),\n",
      " (10,\n",
      "  '0.165*\"saturn\" + -0.149*\"love\" + -0.124*\"agnes\" + -0.120*\"sweet\" + '\n",
      "  '0.113*\"poor\" + -0.109*\"st\" + -0.109*\"joy\" + 0.109*\"still\" + '\n",
      "  '-0.106*\"porphyro\" + 0.103*\"sad\" + -0.098*\"rude\" + -0.096*\"heart\" + '\n",
      "  '0.090*\"thea\" + -0.089*\"madeline\" + 0.088*\"thus\" + 0.083*\"cannot\" + '\n",
      "  '0.077*\"space\" + 0.072*\"sorrow\" + 0.071*\"gods\" + 0.070*\"solemn\"'),\n",
      " (11,\n",
      "  '-0.140*\"saturn\" + -0.114*\"rude\" + 0.111*\"man\" + -0.103*\"new\" + '\n",
      "  '-0.099*\"joys\" + -0.093*\"joy\" + 0.089*\"mind\" + -0.089*\"earth\" + '\n",
      "  '-0.084*\"voice\" + 0.083*\"poor\" + 0.082*\"hand\" + -0.078*\"meadows\" + '\n",
      "  '-0.077*\"come\" + -0.076*\"might\" + -0.076*\"wi\" + -0.075*\"thea\" + '\n",
      "  '-0.074*\"gods\" + 0.069*\"eyes\" + -0.069*\"sky\" + -0.068*\"ear\"'),\n",
      " (12,\n",
      "  '0.201*\"could\" + 0.201*\"misery\" + 0.184*\"thorn\" + 0.155*\"little\" + '\n",
      "  '0.151*\"moss\" + 0.118*\"day\" + 0.116*\"love\" + 0.114*\"pond\" + 0.114*\"hill\" + '\n",
      "  '0.108*\"would\" + 0.100*\"mountain\" + -0.093*\"still\" + 0.093*\"saw\" + '\n",
      "  '0.090*\"infants\" + 0.089*\"heard\" + 0.089*\"dungeon\" + 0.088*\"grave\" + '\n",
      "  '0.087*\"last\" + 0.080*\"know\" + 0.080*\"never\"')]\n",
      "\n",
      "Coherence Score:  0.3501246609173789\n"
     ]
    }
   ],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    '''\n",
    "    Use SVD on the document term matrix and output the LSA model topics\n",
    "    as well as coherence calculated with CoherenceModel\n",
    "    '''\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    print('Document Term Matrix:')\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary, power_iters=50, onepass=False)  # train model\n",
    "    pprint(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    coherencemodel = CoherenceModel(model=lsamodel, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherencemodel.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return lsamodel\n",
    "\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
